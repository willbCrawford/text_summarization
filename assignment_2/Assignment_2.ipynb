{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "The RNN or Recurrent Neural Network will typically have two inputs. One of the inputs represents the $X(t)$ vector. The second input will be $Y_{t-1}(t)$ which is the output at the previous time $t$. The dimensionality of the input can be of any size. The point of RNN's are to allow for high dimensionality time-series models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "Consider  a  CNN  composed of  three  convolutional  layers,  each with  3  ×  3 kernels,  a stride  of  2,  and  \"same\"  padding.  The  lowest layer  outputs  100  feature  maps,  the  middle  one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels. What is the total number of parameters in the CNN? If we are using 32-bit floats,  at  least  how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?\n",
    "-------------------------\n",
    "\n",
    "$Number\\ of\\ parameters\\ = (3 * 3 * 3) * 100 + (3 * 3 * 100 ) * 200 +  (3 * 3 * 200) * 400 = 902,700\\ parameters $\n",
    "\n",
    "\n",
    "$Memory\\ Consumption\\ of\\ single\\ image\\ = \n",
    "    (200 * 300 * 3) * 32 + (100 * 150 * 100) * 32 + (50 * 75 * 200) * 32 + (25 * 37 * 400) * 32 = 89,600,000 * 4 = 358,400,000\\ B = 358.4\\ MB$\n",
    "    \n",
    "$Memory\\ Consumption\\ of\\ minibatch\\ = \n",
    "    ((200 * 300 * 3) * 32) * 50 + ((100 * 150 * 100) * 32) * 50 + ((50 * 75 * 200) * 32) * 50 + ((25 * 37 * 400) * 32) * 50 = 4,480,000,000 * 4 = 17,920,000,000\\ B = 17.92\\ GB$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(directory):\n",
    "    Images = []\n",
    "    Labels = []  # 0 for Building , 1 for forest, 2 for glacier, 3 for mountain, 4 for Sea , 5 for Street\n",
    "    label = 0\n",
    "    \n",
    "    for labels in os.listdir(directory): #Main Directory where each class label is present as folder name.\n",
    "        if labels == 'glacier': #Folder contain Glacier Images get the '2' class label.\n",
    "            label = 2\n",
    "        elif labels == 'sea':\n",
    "            label = 4\n",
    "        elif labels == 'buildings':\n",
    "            label = 0\n",
    "        elif labels == 'forest':\n",
    "            label = 1\n",
    "        elif labels == 'street':\n",
    "            label = 5\n",
    "        elif labels == 'mountain':\n",
    "            label = 3\n",
    "        \n",
    "        for image_file in os.listdir(directory+labels): #Extracting the file name of the image from Class Label folder\n",
    "            image = cv2.imread(directory+labels+r'/'+image_file) #Reading the image (OpenCV)\n",
    "            image = cv2.resize(image,(150,150)) #Resize the image, Some images are different sizes. (Resizing is very Important)\n",
    "            Images.append(image)\n",
    "            Labels.append(label)\n",
    "    \n",
    "    return shuffle(Images,Labels,random_state=817328462) #Shuffle the dataset you just prepared.\n",
    "\n",
    "def get_classlabel(class_code):\n",
    "    labels = {2:'glacier', 4:'sea', 0:'buildings', 1:'forest', 5:'street', 3:'mountain'}\n",
    "    \n",
    "    return labels[class_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images, Labels = get_images('intel-image-classification/seg_train/seg_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images = np.array(Images) #converting the list of images to numpy array.\n",
    "Labels = np.array(Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 148, 148, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 74, 74, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 72, 72, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 70, 70, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 35, 35, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 33, 33, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 31, 31, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 15, 15, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               7372928   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 6)                 66        \n",
      "=================================================================\n",
      "Total params: 8,490,380\n",
      "Trainable params: 8,490,380\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.layers as Layers\n",
    "import tensorflow.keras.activations as Actications\n",
    "import tensorflow.keras.models as Models\n",
    "import tensorflow.keras.optimizers as Optimizer\n",
    "\n",
    "model = Models.Sequential()\n",
    "\n",
    "model.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu',input_shape=(150,150,3)))\n",
    "model.add(Layers.MaxPool2D(pool_size=2))\n",
    "model.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu'))\n",
    "model.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu'))\n",
    "model.add(Layers.MaxPool2D(pool_size=2))\n",
    "model.add(Layers.Conv2D(256,kernel_size=(3,3),activation='relu'))\n",
    "model.add(Layers.Conv2D(256,kernel_size=(3,3),activation='relu'))\n",
    "model.add(Layers.MaxPool2D(pool_size=2))\n",
    "model.add(Layers.Flatten())\n",
    "model.add(Layers.Dense(128,activation='relu'))\n",
    "model.add(Layers.Dense(64,activation='relu'))\n",
    "model.add(Layers.Dense(10,activation='relu'))\n",
    "model.add(Layers.Dropout(rate=0.5))\n",
    "model.add(Layers.Dense(6,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Optimizer.Adam(lr=0.0001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12630 samples, validate on 1404 samples\n",
      "Epoch 1/50\n",
      "  416/12630 [..............................] - ETA: 25:05 - loss: 1.7918 - acc: 0.1587"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-063073dbd869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(Images, Labels, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
