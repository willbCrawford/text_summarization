{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f4921dfda30b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m \u001b[0;31m# linear algebra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[0;31m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import spacy\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import display\n",
    "import base64\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from time import time\n",
    "# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import heapq\n",
    "%matplotlib inline\n",
    "\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "def normalize_text(text):\n",
    "    return text.replace(\"\\n\", \" \")\n",
    "\n",
    "summ_article = {}\n",
    "summ_article['article'] = []\n",
    "summ_article['summary'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for articles, summaries in zip(os.walk('input/BBC News Summary/News Articles/business'), os.walk('input/BBC News Summary/Summaries/business')):\n",
    "    for filename in zip(articles[2], summaries[2]):\n",
    "        with open(os.path.join(articles[0], filename[0])) as fh:\n",
    "            file = \"\"\n",
    "            for line in fh:\n",
    "                file += line\n",
    "            summ_article['article'].append(file)\n",
    "            \n",
    "        with open(os.path.join(summaries[0], filename[1])) as fh:\n",
    "            file = \"\"\n",
    "            for line in fh:\n",
    "                file += line\n",
    "            summ_article['summary'].append(file)\n",
    "            \n",
    "for articles, summaries in zip(os.walk('input/BBC News Summary/News Articles/entertainment'), os.walk('input/BBC News Summary/Summaries/entertainment')):\n",
    "    for filename in zip(articles[2], summaries[2]):\n",
    "        with open(os.path.join(articles[0], filename[0])) as fh:\n",
    "            file = \"\"\n",
    "            for line in fh:\n",
    "                file += line\n",
    "            summ_article['article'].append(file)\n",
    "            \n",
    "        with open(os.path.join(summaries[0], filename[1])) as fh:\n",
    "            file = \"\"\n",
    "            for line in fh:\n",
    "                file += line\n",
    "            summ_article['summary'].append(file)\n",
    "\n",
    "for articles, summaries in zip(os.walk('input/BBC News Summary/News Articles/politics'), os.walk('input/BBC News Summary/Summaries/politics')):\n",
    "    for filename in zip(articles[2], summaries[2]):\n",
    "        with open(os.path.join(articles[0], filename[0])) as fh:\n",
    "            file = \"\"\n",
    "            for line in fh:\n",
    "                file += line\n",
    "            summ_article['article'].append(file)\n",
    "            \n",
    "        with open(os.path.join(summaries[0], filename[1])) as fh:\n",
    "            file = \"\"\n",
    "            for line in fh:\n",
    "                file += line\n",
    "            summ_article['summary'].append(file)\n",
    "            \n",
    "for articles, summaries in zip(os.walk('input/BBC News Summary/News Articles/sport'), os.walk('input/BBC News Summary/Summaries/sport')):\n",
    "    for filename in zip(articles[2], summaries[2]):\n",
    "        with open(os.path.join(articles[0], filename[0]), encoding=\"utf8\", errors='ignore') as fh:\n",
    "            file = \"\"\n",
    "            for line in fh:\n",
    "                file += line\n",
    "            summ_article['article'].append(file)\n",
    "            \n",
    "        with open(os.path.join(summaries[0], filename[1])) as fh:\n",
    "            file = \"\"\n",
    "            for line in fh:\n",
    "                file += line\n",
    "            summ_article['summary'].append(file)\n",
    "            \n",
    "for articles, summaries in zip(os.walk('input/BBC News Summary/News Articles/tech'), os.walk('input/BBC News Summary/Summaries/tech')):\n",
    "    for filename in zip(articles[2], summaries[2]):\n",
    "        with open(os.path.join(articles[0], filename[0])) as fh:\n",
    "            file = \"\"\n",
    "            for line in fh:\n",
    "                file += line\n",
    "            summ_article['article'].append(file)\n",
    "            \n",
    "        with open(os.path.join(summaries[0], filename[1])) as fh:\n",
    "            file = \"\"\n",
    "            for line in fh:\n",
    "                file += line\n",
    "            summ_article['summary'].append(file)\n",
    "\n",
    "df = pd.DataFrame(data=summ_article)\n",
    "\n",
    "df.columns = ['article', 'summary']\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_article_first_pass'] = df['article'].apply(normalize_text)\n",
    "df['cleaned_summary_first_pass'] = df['summary'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "      <th>cleaned_article_first_pass</th>\n",
       "      <th>cleaned_summary_first_pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UK economy facing 'major risks'\\n\\nThe UK manu...</td>\n",
       "      <td>\"Despite some positive news for the export sec...</td>\n",
       "      <td>UK economy facing 'major risks'  The UK manufa...</td>\n",
       "      <td>\"Despite some positive news for the export sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aids and climate top Davos agenda\\n\\nClimate c...</td>\n",
       "      <td>At the same time, about 100,000 people are exp...</td>\n",
       "      <td>Aids and climate top Davos agenda  Climate cha...</td>\n",
       "      <td>At the same time, about 100,000 people are exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asian quake hits European shares\\n\\nShares in ...</td>\n",
       "      <td>The unfolding scale of the disaster in south A...</td>\n",
       "      <td>Asian quake hits European shares  Shares in Eu...</td>\n",
       "      <td>The unfolding scale of the disaster in south A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India power shares jump on debut\\n\\nShares in ...</td>\n",
       "      <td>Shares in India's largest power producer, Nati...</td>\n",
       "      <td>India power shares jump on debut  Shares in In...</td>\n",
       "      <td>Shares in India's largest power producer, Nati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lacroix label bought by US firm\\n\\nLuxury good...</td>\n",
       "      <td>LVMH said the French designer's haute couture ...</td>\n",
       "      <td>Lacroix label bought by US firm  Luxury goods ...</td>\n",
       "      <td>LVMH said the French designer's haute couture ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  UK economy facing 'major risks'\\n\\nThe UK manu...   \n",
       "1  Aids and climate top Davos agenda\\n\\nClimate c...   \n",
       "2  Asian quake hits European shares\\n\\nShares in ...   \n",
       "3  India power shares jump on debut\\n\\nShares in ...   \n",
       "4  Lacroix label bought by US firm\\n\\nLuxury good...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  \"Despite some positive news for the export sec...   \n",
       "1  At the same time, about 100,000 people are exp...   \n",
       "2  The unfolding scale of the disaster in south A...   \n",
       "3  Shares in India's largest power producer, Nati...   \n",
       "4  LVMH said the French designer's haute couture ...   \n",
       "\n",
       "                          cleaned_article_first_pass  \\\n",
       "0  UK economy facing 'major risks'  The UK manufa...   \n",
       "1  Aids and climate top Davos agenda  Climate cha...   \n",
       "2  Asian quake hits European shares  Shares in Eu...   \n",
       "3  India power shares jump on debut  Shares in In...   \n",
       "4  Lacroix label bought by US firm  Luxury goods ...   \n",
       "\n",
       "                          cleaned_summary_first_pass  \n",
       "0  \"Despite some positive news for the export sec...  \n",
       "1  At the same time, about 100,000 people are exp...  \n",
       "2  The unfolding scale of the disaster in south A...  \n",
       "3  Shares in India's largest power producer, Nati...  \n",
       "4  LVMH said the French designer's haute couture ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~©'\n",
    "# Define function to cleanup text by removing personal pronouns, stopwords, and puncuation\n",
    "def cleanup_text(docs, logging=False):\n",
    "    texts = []\n",
    "    doc = nlp(docs, disable=['parser', 'ner'])\n",
    "    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "    tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
    "    tokens = ' '.join(tokens)\n",
    "    texts.append(tokens)\n",
    "    return pd.Series(texts)\n",
    "\n",
    "df['article_cleaned'] = df['cleaned_article_first_pass'].apply(lambda x: cleanup_text(x, False))\n",
    "df['summary_cleaned'] = df['cleaned_summary_first_pass'].apply(lambda x: cleanup_text(x, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "      <th>cleaned_article_first_pass</th>\n",
       "      <th>cleaned_summary_first_pass</th>\n",
       "      <th>article_cleaned</th>\n",
       "      <th>summary_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UK economy facing 'major risks'\\n\\nThe UK manu...</td>\n",
       "      <td>\"Despite some positive news for the export sec...</td>\n",
       "      <td>UK economy facing 'major risks'  The UK manufa...</td>\n",
       "      <td>\"Despite some positive news for the export sec...</td>\n",
       "      <td>uk economy face major risk uk manufacturing se...</td>\n",
       "      <td>despite positive news export sector worry sign...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aids and climate top Davos agenda\\n\\nClimate c...</td>\n",
       "      <td>At the same time, about 100,000 people are exp...</td>\n",
       "      <td>Aids and climate top Davos agenda  Climate cha...</td>\n",
       "      <td>At the same time, about 100,000 people are exp...</td>\n",
       "      <td>aid climate top davos agenda climate change fi...</td>\n",
       "      <td>time 100,000 people expect converge brazilian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asian quake hits European shares\\n\\nShares in ...</td>\n",
       "      <td>The unfolding scale of the disaster in south A...</td>\n",
       "      <td>Asian quake hits European shares  Shares in Eu...</td>\n",
       "      <td>The unfolding scale of the disaster in south A...</td>\n",
       "      <td>asian quake hit european share share europe 's...</td>\n",
       "      <td>unfold scale disaster south asia little immedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India power shares jump on debut\\n\\nShares in ...</td>\n",
       "      <td>Shares in India's largest power producer, Nati...</td>\n",
       "      <td>India power shares jump on debut  Shares in In...</td>\n",
       "      <td>Shares in India's largest power producer, Nati...</td>\n",
       "      <td>india power share jump debut share india 's la...</td>\n",
       "      <td>share india 's large power producer national t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lacroix label bought by US firm\\n\\nLuxury good...</td>\n",
       "      <td>LVMH said the French designer's haute couture ...</td>\n",
       "      <td>Lacroix label bought by US firm  Luxury goods ...</td>\n",
       "      <td>LVMH said the French designer's haute couture ...</td>\n",
       "      <td>lacroix label buy us firm luxury good group lv...</td>\n",
       "      <td>lvmh say french designer 's haute couture read...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  UK economy facing 'major risks'\\n\\nThe UK manu...   \n",
       "1  Aids and climate top Davos agenda\\n\\nClimate c...   \n",
       "2  Asian quake hits European shares\\n\\nShares in ...   \n",
       "3  India power shares jump on debut\\n\\nShares in ...   \n",
       "4  Lacroix label bought by US firm\\n\\nLuxury good...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  \"Despite some positive news for the export sec...   \n",
       "1  At the same time, about 100,000 people are exp...   \n",
       "2  The unfolding scale of the disaster in south A...   \n",
       "3  Shares in India's largest power producer, Nati...   \n",
       "4  LVMH said the French designer's haute couture ...   \n",
       "\n",
       "                          cleaned_article_first_pass  \\\n",
       "0  UK economy facing 'major risks'  The UK manufa...   \n",
       "1  Aids and climate top Davos agenda  Climate cha...   \n",
       "2  Asian quake hits European shares  Shares in Eu...   \n",
       "3  India power shares jump on debut  Shares in In...   \n",
       "4  Lacroix label bought by US firm  Luxury goods ...   \n",
       "\n",
       "                          cleaned_summary_first_pass  \\\n",
       "0  \"Despite some positive news for the export sec...   \n",
       "1  At the same time, about 100,000 people are exp...   \n",
       "2  The unfolding scale of the disaster in south A...   \n",
       "3  Shares in India's largest power producer, Nati...   \n",
       "4  LVMH said the French designer's haute couture ...   \n",
       "\n",
       "                                     article_cleaned  \\\n",
       "0  uk economy face major risk uk manufacturing se...   \n",
       "1  aid climate top davos agenda climate change fi...   \n",
       "2  asian quake hit european share share europe 's...   \n",
       "3  india power share jump debut share india 's la...   \n",
       "4  lacroix label buy us firm luxury good group lv...   \n",
       "\n",
       "                                     summary_cleaned  \n",
       "0  despite positive news export sector worry sign...  \n",
       "1  time 100,000 people expect converge brazilian ...  \n",
       "2  unfold scale disaster south asia little immedi...  \n",
       "3  share india 's large power producer national t...  \n",
       "4  lvmh say french designer 's haute couture read...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article_len'] = df['article_cleaned'].str.split().str.len()\n",
    "df['summary_len'] = df['summary_cleaned'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "      <th>cleaned_article_first_pass</th>\n",
       "      <th>cleaned_summary_first_pass</th>\n",
       "      <th>article_cleaned</th>\n",
       "      <th>summary_cleaned</th>\n",
       "      <th>article_len</th>\n",
       "      <th>summary_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UK economy facing 'major risks'\\n\\nThe UK manu...</td>\n",
       "      <td>\"Despite some positive news for the export sec...</td>\n",
       "      <td>UK economy facing 'major risks'  The UK manufa...</td>\n",
       "      <td>\"Despite some positive news for the export sec...</td>\n",
       "      <td>uk economy face major risk uk manufacturing se...</td>\n",
       "      <td>despite positive news export sector worry sign...</td>\n",
       "      <td>219</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aids and climate top Davos agenda\\n\\nClimate c...</td>\n",
       "      <td>At the same time, about 100,000 people are exp...</td>\n",
       "      <td>Aids and climate top Davos agenda  Climate cha...</td>\n",
       "      <td>At the same time, about 100,000 people are exp...</td>\n",
       "      <td>aid climate top davos agenda climate change fi...</td>\n",
       "      <td>time 100,000 people expect converge brazilian ...</td>\n",
       "      <td>294</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asian quake hits European shares\\n\\nShares in ...</td>\n",
       "      <td>The unfolding scale of the disaster in south A...</td>\n",
       "      <td>Asian quake hits European shares  Shares in Eu...</td>\n",
       "      <td>The unfolding scale of the disaster in south A...</td>\n",
       "      <td>asian quake hit european share share europe 's...</td>\n",
       "      <td>unfold scale disaster south asia little immedi...</td>\n",
       "      <td>401</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India power shares jump on debut\\n\\nShares in ...</td>\n",
       "      <td>Shares in India's largest power producer, Nati...</td>\n",
       "      <td>India power shares jump on debut  Shares in In...</td>\n",
       "      <td>Shares in India's largest power producer, Nati...</td>\n",
       "      <td>india power share jump debut share india 's la...</td>\n",
       "      <td>share india 's large power producer national t...</td>\n",
       "      <td>127</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lacroix label bought by US firm\\n\\nLuxury good...</td>\n",
       "      <td>LVMH said the French designer's haute couture ...</td>\n",
       "      <td>Lacroix label bought by US firm  Luxury goods ...</td>\n",
       "      <td>LVMH said the French designer's haute couture ...</td>\n",
       "      <td>lacroix label buy us firm luxury good group lv...</td>\n",
       "      <td>lvmh say french designer 's haute couture read...</td>\n",
       "      <td>111</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  UK economy facing 'major risks'\\n\\nThe UK manu...   \n",
       "1  Aids and climate top Davos agenda\\n\\nClimate c...   \n",
       "2  Asian quake hits European shares\\n\\nShares in ...   \n",
       "3  India power shares jump on debut\\n\\nShares in ...   \n",
       "4  Lacroix label bought by US firm\\n\\nLuxury good...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  \"Despite some positive news for the export sec...   \n",
       "1  At the same time, about 100,000 people are exp...   \n",
       "2  The unfolding scale of the disaster in south A...   \n",
       "3  Shares in India's largest power producer, Nati...   \n",
       "4  LVMH said the French designer's haute couture ...   \n",
       "\n",
       "                          cleaned_article_first_pass  \\\n",
       "0  UK economy facing 'major risks'  The UK manufa...   \n",
       "1  Aids and climate top Davos agenda  Climate cha...   \n",
       "2  Asian quake hits European shares  Shares in Eu...   \n",
       "3  India power shares jump on debut  Shares in In...   \n",
       "4  Lacroix label bought by US firm  Luxury goods ...   \n",
       "\n",
       "                          cleaned_summary_first_pass  \\\n",
       "0  \"Despite some positive news for the export sec...   \n",
       "1  At the same time, about 100,000 people are exp...   \n",
       "2  The unfolding scale of the disaster in south A...   \n",
       "3  Shares in India's largest power producer, Nati...   \n",
       "4  LVMH said the French designer's haute couture ...   \n",
       "\n",
       "                                     article_cleaned  \\\n",
       "0  uk economy face major risk uk manufacturing se...   \n",
       "1  aid climate top davos agenda climate change fi...   \n",
       "2  asian quake hit european share share europe 's...   \n",
       "3  india power share jump debut share india 's la...   \n",
       "4  lacroix label buy us firm luxury good group lv...   \n",
       "\n",
       "                                     summary_cleaned  article_len  summary_len  \n",
       "0  despite positive news export sector worry sign...          219           89  \n",
       "1  time 100,000 people expect converge brazilian ...          294          166  \n",
       "2  unfold scale disaster south asia little immedi...          401          183  \n",
       "3  share india 's large power producer national t...          127           53  \n",
       "4  lvmh say french designer 's haute couture read...          111           40  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "rev = df.groupby('article_len')['summary_len'].mean().reset_index()\n",
    "# Using set_dashes() to modify dashing of an existing line\n",
    "line1 = ax.scatter(rev['summary_len'], rev['article_len'])\n",
    "\n",
    "ax.xlabel()\n",
    "plt.savefig('article_summary_len.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "graph_df= pd.DataFrame()\n",
    "graph_df['article']=df['article_len']\n",
    "graph_df['summary']=df['summary_len']\n",
    "\n",
    "graph_df.hist(bins = 20)\n",
    "\n",
    "plt.savefig('article_summary_histogram.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is function for text summarization\n",
    "def generate_summary(text_without_removing_dot, cleaned_text):\n",
    "    sample_text = text_without_removing_dot\n",
    "    doc = nlp(sample_text)\n",
    "    sentence_list=[]\n",
    "    for idx, sentence in enumerate(doc.sents): # we are using spacy for sentence tokenization\n",
    "        sentence_list.append(re.sub(r'[^\\w\\s]','',str(sentence)))\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    word_frequencies = {}  \n",
    "    for word in nltk.word_tokenize(cleaned_text):  \n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "\n",
    "\n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():  \n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "\n",
    "    sentence_scores = {}  \n",
    "    for sent in sentence_list:  \n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "\n",
    "    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    print(\"Original Text:\\n\")\n",
    "    print(text_without_removing_dot)\n",
    "    print('\\n\\nSummarized text:\\n')\n",
    "    print(summary)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "WorldCom director admits lying  The former chief financial officer at US telecoms firm WorldCom has admitted before a New York court that he used to lie to fellow board members.  Speaking at the trial of his former boss Bernard Ebbers, Scott Sullivan said he lied to the board to cover up the hole in WorldCom's finances. Mr Ebbers is on trial for fraud and conspiracy in relation to WorldCom's collapse in 2002. He pleads not guilty. The firm had been overstating its accounts by $11bn (£8.5bn). Mr Sullivan, 42, has already pleaded guilty to fraud and will be sentenced following Mr Ebbers' trial, where he is appearing as a prosecution witness. Mr Ebbers, 63, has always insisted that he was unaware of any hidden shortfalls in WorldCom's finances.  In the New York court on Wednesday, Mr Ebbers' lawyer Reid Weingarten asked Mr Sullivan: \"If you believe something is in your interest, you are willing and able to lie to accomplish it, isn't that right?\"  \"On that date, yes. I was lying,\" replied Mr Sullivan. Mr Weingarten has suggested that Mr Sullivan is implicating Mr Ebbers only to win a lighter sentence, something Mr Sullivan denies. Mr Sullivan also rejects a suggestion that he had once told fellow WorldCom board member Bert Roberts that Mr Ebbers was unaware of the accounting fraud at WorldCom. The trial of Mr Ebbers is now into its third week.  Under 23 hours of questioning from a federal prosecutor, Mr Sullivan has previously told the court that he repeatedly warned Mr Ebbers that falsifying the books would be the only way to meet Wall Street revenue and earnings expectations. Mr Sullivan claims that Mr Ebbers refused to stop the fraud. Mr Ebbers could face a sentence of 85 years if convicted of all the charges he is facing. WorldCom's problems appear to have begun with the collapse of the dotcom boom which cut its business from internet companies. Prosecutors allege that the company's top executives responded by orchestrating massive fraud over a two-year period. WorldCom emerged from bankruptcy protection in 2004, and is now known as MCI. \n",
      "\n",
      "\n",
      "Summarized text:\n",
      "\n",
      "Mr Weingarten has suggested that Mr Sullivan is implicating Mr Ebbers only to win a lighter sentence something Mr Sullivan denies Mr Sullivan also rejects a suggestion that he had once told fellow WorldCom board member Bert Roberts that Mr Ebbers was unaware of the accounting fraud at WorldCom Mr Sullivan 42 has already pleaded guilty to fraud and will be sentenced following Mr Ebbers trial where he is appearing as a prosecution witness Mr Sullivan claims that Mr Ebbers refused to stop the fraud Mr Ebbers is on trial for fraud and conspiracy in relation to WorldComs collapse in 2002 Mr Ebbers could face a sentence of 85 years if convicted of all the charges he is facing The trial of Mr Ebbers is now into its third week  \n",
      "\n",
      "\n",
      " Original Summary:\n",
      "\n",
      "mr sullivan 42 already plead guilty fraud sentence follow mr ebbers trial appear prosecution witness . mr sullivan claim mr ebbers refuse stop fraud . mr weingarten suggest mr sullivan implicate mr ebbers win light sentence something mr sullivan deny . mr sullivan also reject suggestion tell fellow worldcom board member bert roberts mr ebbers unaware accounting fraud worldcom . mr ebbers trial fraud conspiracy relation worldcom 's collapse 2002.the trial mr ebbers third week . new york court wednesday mr ebbers lawyer reid weingarten ask mr sullivan believe something interest willing able lie accomplish right?\"mr ebbers 63 always insist unaware hidden shortfall worldcom 's finance .\n"
     ]
    }
   ],
   "source": [
    "generate_summary(df['cleaned_article_first_pass'][8], df['article_cleaned'][8])\n",
    "\n",
    "print('\\n\\n Original Summary:\\n')\n",
    "print(df['summary_cleaned'][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(np.array(df['cleaned_article_first_pass']),\n",
    "                                            np.array(df['cleaned_summary_first_pass']),\n",
    "                                            test_size=0.1,\n",
    "                                            random_state=0,\n",
    "                                            shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 18.50128068969826\n",
      "Total Coverage of rare words: 0.3149706946204503\n"
     ]
    }
   ],
   "source": [
    "thresh=4\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary in X = 26092\n"
     ]
    }
   ],
   "source": [
    "max_text_len=100\n",
    "max_summary_len=100\n",
    "\n",
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences (i.e one-hot encodeing all the words)\n",
    "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "x_voc   =  x_tokenizer.num_words + 1\n",
    "\n",
    "print(\"Size of vocabulary in X = {}\".format(x_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 34.069913188802744\n",
      "Total Coverage of rare words: 1.5318182932651052\n"
     ]
    }
   ],
   "source": [
    "thresh=6\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary in Y = 14203\n"
     ]
    }
   ],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "#convert text sequences into integer sequences (i.e one hot encode the text in Y)\n",
    "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "#size of vocabulary\n",
    "y_voc  =   y_tokenizer.num_words +1\n",
    "print(\"Size of vocabulary in Y = {}\".format(y_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary from the w2v model = 26092\n",
      "WARNING:tensorflow:From /Users/willcrawford/opt/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/willcrawford/opt/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 100, 200)     5218400     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 100, 300), ( 601200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 100, 300), ( 721200      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    2840600     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 100, 300), ( 721200      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 300),  601200      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 14203)  4275103     lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 14,978,903\n",
      "Trainable params: 14,978,903\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "import gensim\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Size of vocabulary from the w2v model = {}\".format(x_voc))\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim=200\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/willcrawford/opt/anaconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 8285 samples, validate on 921 samples\n",
      "Epoch 1/50\n",
      " 256/8285 [..............................] - ETA: 24:40 - loss: 9.5575"
     ]
    }
   ],
   "source": [
    "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=50,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
